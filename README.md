# tiny-transformer
minimal multi-head attention
